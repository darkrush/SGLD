\section{Conclusion and Further work}
This article summarizes the reasons that lead to difficulties in exploring sparse/deceptive tasks as "zero rewards" and "action penalty" and indicates how these factors affect reinforcement learning algorithms. We propose an exploration strategy using SGLD to achieve Thompson sampling in the actor-critic algorithm, which overcomes the shortcomings of existing algorithms and achieve effectively explore in sparse/deceptive tasks, with no additional hyper-parameters are introduced and is computationally friendly. We evaluate our method on all three continuous tasks in the same settings, and the results show that our method can solve more tasks.

Our work demonstrates the exploration strategy with a combination of parameter noise and posterior sampling in reinforcement learning. This algorithm continuously generates the sampling of the value function under all observed data and the corresponding policy during the training process. This approach may combine with evolutionary algorithms as a way to generate new populations, which may cause cover the policy parameter space better and make full use of the data for each time step. Also, the intensity of exploration in our approach is related to the number of data observed, which means throwing away some of the redundant data will "restart" the exploration, combined with further data management strategy design may lead to better exploration.