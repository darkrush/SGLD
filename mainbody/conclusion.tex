\section{Conclusion and Further work}
This article summarizes the reasons that lead to difficulties in exploring sparse/depetive tasks as "zero reward" and "action penalty" and indicates how these factors affect reinforcement learning algorithms. We propose an exploring strategies using SGLD to achieve Tompson sampling in actor-critic algorithm, which overcomes the shortcomings of existing algorithms and achieve effectively explore in sparse/deceptive tasks, with no additional hyperparameters are introduced and is computationally friendly. We use the same settings to evaluate our method on three continous tasks, and the results show that our method can solve more tasks and the sample is more efficient.

Our work demonstrates the exploration strategy with combination of parameter noise and posterior sampling in reinforcement learning. This algorithm continuously generates the sampling of the value function under all observed data and the corresponding policy during the training process. This approach may be combined with evolution algorithms as a way to generate new populations, which may cause cover the policy parameter space better and make full use of the data for each time step. In addition, the intensity of exploration in our approach is related to the number of data observed, which means throwing away some of the redundant data will "restart" the exploration, combined with further data management strategy design may lead to better exploration.