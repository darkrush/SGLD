\section{Introduction}
The application of deep neural networks in reinforcement learning has made appealing progress \cite{DQN,AlphaGO,OpenAIdota}. Deep neural networks serve as flexible function approximators, which enables reinforcement learning to solve various challenging tasks \cite{DQN,RN395}, e.g., the ones with pixel input using convolutional neural networks, and the ones that lack Markov property using recurrent neural networks. The latest generation of deep learning frameworks has convenient automatic derivation and integrates various optimizers \cite{PyTorch,MXNet,TF}. As in supervised learning,  ``loss function specification + stochastic gradient-based optimizer'' has become the standard approach to apply deep neural networks in reinforcement learning, both for Q-learning and policy gradient \cite{DDPG,DQN,PPO}.

However, in more difficult tasks, especially those with continuous state-action space and sparse or deceptive rewards, more effective exploration strategies become the key to success \cite{pnoise,colas2018gep}. In tasks with sparse rewards, unless a few specific events are triggered, the agent consistently witnesses zero rewards, which makes it necessary for the agent to traverse the policy space effectively, even without particular incentives \cite{VIME}. In tasks with deceptive rewards, due to the action penalty in the rewards, the agent consistently receives negative feedback during exploration, which renders the agent unable to explore and consequently choose to maintain the default action \cite{lehman2011abandoning,conti2018improving}. Such a situation makes it necessary for the agent to continue to explore the unknown part of the state-action space. Conversely, in the tasks with well-defined rewards, which can guide the agent to learn the optimal strategy, too much exploration may hurt the sample efficiency \cite{Showdown}. Such a trade-off between exploration and exploitation plays a vital role in reinforcement learning.

There has been a line of works on exploration strategies, including but not limited to random search (over action or parameter space) \cite{pnoise,DDPG}, count-based methods \cite{count1,count2}, and evolutionary strategies \cite{EPGRL,ERL2}. In particular, there is a class of exploration strategies based on Thompson sampling \cite{TS}, the core idea of which is to make greedy decisions based on the posterior distribution of value function. Thompson sampling has an ``instance-independent regret bound” \cite{TStutorial} and has been proven to be effective in a variety of deep reinforcement learning scenarios \cite{BDQN,VIME,dropoutInference,lastLayerBayes}.

However, when using deep neural networks to parametrize value functions, it is often intractable to calculate the posterior distributions in closed form. To address such a challenge, there has been a line of works on achieving posterior sampling through various approximation methods, including dropout \cite{dropoutInference}, variational inference \cite{VIME}, bootstrapping \cite{BDQN}, and so on. However, the posterior distributions obtained by these methods are often inaccurate, and moreover, may lack intrinsic motivation and suffer high computational cost \cite{osband2018randomized}.

Note that the optimization of value functions in reinforcement learning has many similarities with the optimization of loss functions in supervised learning. Such a key observation motivates us to use a stochastic gradient-based Monte Carlo sampler --- namely, Stochastic Gradient Langevin Dynamics (SGLD) --- to achieve posterior sampling \cite{SGLD}. In particular, we incorporate SGLD into the standard stochastic gradient-based optimizer to continuously generate posterior samples based on previously obtained observations. Then we use such samples to generate the actors for exploration during the rollout phase.

In this work, we use the Deep Deterministic Policy Gradient (DDPG) algorithm \cite{DDPG} as an example algorithm. Then we evaluate the performance of our exploration strategy with Continuous-Mountain-Car, Sparse-Half-Cheetah, and Half-Cheetah as deceptive, sparse, and well-defined tasks, and compare it against existing approaches based action noise and parameter noise. In general, the SGLD-based method can solve a broader variety of tasks with a single configuration. We summarize three major highlights of this work:
\begin{itemize}
\item  It is the first time to propose the use of SGLD to achieve Thompson sampling in the actor-critic algorithm, which uses deep neural networks as value functions.
\item We identify two difficulties of exploration: sparse reward and deceptive reward. The experimental results prove that our strategy can solve different difficult tasks with a single configuration.
\item  Our exploration strategy is application-friendly as it introduces no extra hyper-parameter, and has almost no additional computational overhead. 
\end{itemize} 

This paper is organized as follows: In Section 2, we analyze how the “zero reward” and “action penalty” issues in the environment affect the actor-critic algorithm and review the works on relevant exploration strategies. Section 3 briefly introduces SGLD and explains how to use SGLD to sample critics following the posterior distribution, and how to incorporate exploration into the actor-critic algorithm. In Section 4, we implement SGLD-based exploration strategies using DDPG as a baseline algorithm and evaluate them in several different environments. Then the results are compared with the baseline, action noise, and parameter noise. Finally, we summarize this work in Section 5 and point out several future directions.
