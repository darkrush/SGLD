\section{Introduction}
The application of deep learning in reinforcement learning has made an appealing progress \cite{DQN,AlphaGO,OpenAIdota}. Neural networks can fit arbitrary functions and have many extended forms on different tasks, which enables reinforcement learning to solve various tasks \cite{DQN,RN395}, such as  for pixel input and RNN for non-Markov tasks. The latest generation of deep learning frameworks has convenient automatic derivation and integrates various optimizers \cite{PyTorch,MXNet,TF}. As in supervised learning,  "loss function + SGD-based optimizer" has become the most general approach to apply neural networks in reinforcement learning, both for Q-learning or policy gradient \cite{DDPG,DQN,PPO}.

However, in more difficult tasks, especially those with continuous state-action space, or sparse/deceptive rewards, more effective exploration strategies become the key to solving the problem \cite{pnoise,colas2018gep}. In sparse reward tasks, except for a few specific events, the agent can only get 0 rewards, which will require the agent to traverse the policy space effectively, even without any incentives\cite{VIME}. In deceptive reward tasks, due to the action penalty in the reward, the agent will receive negative feedback during exploring, which leads the agent to be unable to explore and maintain the "zero action" \cite{lehman2011abandoning,conti2018improving}. This situation requires the agent to continue to explore the unknown part of the state action space. Conversely, in the tasks with well-defined reward, which can guide the agent to learn the optimal strategy, too much exploration will hurt the sampling efficiency \cite{Showdown}. The trade-off between exploration and exploitation is an important research issue for reinforcement learning.

There has been a lot of research work on the agent's exploration strategy, including some naive random search methods (action space or parameter space) \cite{pnoise,DDPG}, count-based methods \cite{count1,count2}, evolutionary strategy \cite{EPGRL,ERL2} and so on. There is a class of methods based on Thomson's sampling \cite{TS}, the core idea of which is to make decisions using samples generated from the posterior distribution of the value model on the current data. Thompson sampling has an “Instance-Independent Regret bounds” \cite{TStutorial} and has proven to be effective in a variety of deep learning-based reinforcement learning scenarios \cite{BDQN,VIME,dropoutInference,lastLayerBayes}.

However, for an algorithm that uses a neural network as a function of value, it is difficult to calculate the posterior distribution directly. Building on this consideration, there are a series of works to achieve posterior sampling using various approximation methods, including dropout \cite{dropoutInference}, variational inference \cite{VIME}, Bootstrap \cite{BDQN}, and so on. However, the posterior distributions obtained by these methods are not accurate, and there may be problems such as lack of intrinsic motivation and computational cost. \cite{osband2018randomized}

It is noted that the training process of optimizing the value function in reinforcement learning has many similarities with the supervised learning. We use a stochastic gradient based Monte Carlo sampler: Stochastic Gradient Langevin Dynamics (SGLD) to achieve posterior sampling \cite{SGLD}. We use the SGLD sampler instead of the value estimate function's optimizer to continuously generate posterior samples of the current observations during the training step. Then we use these samples to generate policy gradient optimization actors for exploration during the rollout phase.

In this work, we use the Deep Deterministic Policy Gradient (DDPG) algorithm as the example algorithm \cite{DDPG}. Then evaluate the performance of our exploration method with Continuous MountainCar, Sparse HalfCheetah, and HalfCheetah as deceptive, sparse, and well-defined tasks, and compared it with Action noise, Parameter noise. In general, the SGLD-based method can solve a wider variety of tasks with uniform settings. We summarize three major highlights of this work:
\begin{itemize}
\item  It is the first time to propose that the use of the SGLD method to achieve Thompson sampling in the actor-critic algorithm which use neural network as value function.
\item We summarize two reasons for the difficulty of exploration: sparse reward and action penalty. The experimental results prove that our method can solve different difficult tasks by uniform settings.
\item  Our method is application friendly, because it introduces no extra hyper-parameter, and has almost no additional computational overhead. 
\end{itemize} 

This paper is organized as follows: In section 2, we analyzed how the “zero reward” and “action penalty” issues in the environment affect the actor-critic algorithm and explore the work of the relevant exploration strategies. The third section briefly introduces the SGLD method and explains how to use the SGLD to sample critic follow posterior distribution and how to do exploration for the actor-critic algorithm. In Section 4, we implemented SGLD-based exploration strategies using DDPG as a baseline algorithm and evaluated them in several different environments. Then the the results are compared with baseline, action noise, and parameter noise. Finally, we summarize this work in section 5 and point out our opinions on further research work.
