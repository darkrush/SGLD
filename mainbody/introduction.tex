\section{Introduction}
The application of deep learning in reinforcement learning has brought a lot of research progress \cite{DQN,AlphaGO,OpenAIdota}. Neural networks can fit arbitrary functions and have many extended forms on different tasks, which enabling reinforcement learning to solve various tasks, such as CNN for pixel input and RNN for non-Markov tasks\cite{DQN,RN395}. The latest generation of deep learning frameworks has convenient automatic derivation, integrate various optimizers \cite{PyTorch,MXNet,TF}. As in supervised learning, the "loss function + SGD-base optimizer" approach becomes the general way of using neural networks in RL, both for Q-learning or policy gradient \cite{DDPG,DQN,PPO}.

However, in more difficult tasks, especially those with continuous state-action space, or sparse/deceptive rewards, more effective exploration strategies become the key to solving the problem \cite{pnoise,colas2018gep}. In the sparse reward task, except for a few specific events, the agent can only get 0 rewards, which will require the agent to effectively traverse the policy space without any incentives \cite{VIME}. In the deceptive reward task, due to the action penalty in the reward, the agent will receive negative feedback when exploring, which leads the agent to abandon the exploration and maintain the "zero action" \cite{lehman2011abandoning,conti2018improving}. This situation requires the agent to continue to explore the unknown part of the state action space. In the opposite case, if the task has a well-defined reward, which can guide the agent to learn the optimal strategy, too much exploration will hurt the sample efficiency \cite{Showdown}. The trade-off between exploration and exploitation is an important research issue for reinforcement learning.

There has been a lot of research work on the agent's exploration strategy, including some naive random search methods (action space or parameter space) \cite{pnoise,DDPG}, count-based methods \cite{count1,count2}, evolutionary strategy \cite{EPGRL,ERL2} and so on. There is a class of methods based on Thomson's sampling \cite{TS}, the core idea of which is to make decisions using samples generated from the posterior distribution of the value model on the current data. Thompson sampling has an “Instance-Independent Regret bounds” \cite{TStutorial} and has proven to be effective in a variety of deep learning-based reinforcement learning scenarios \cite{BDQN,VIME,dropoutInference,lastLayerBayes}.

However, for an algorithm that uses a neural network as a function of value estimation, it is difficult to directly calculate the posterior distribution. In response to this, there are a series of works to achieve approximate a posteriori sampling using various approximation methods, including dropout \cite{dropoutInference}, variational inference \cite{VIME}, Bootstrap \cite{BDQN}, and so on. However, the posterior distributions followed by these methods are not accurate, and there may be problems such as lack of intrinsic motivation and computational cost. \cite{osband2018randomized}

It is noted that the training process of optimizing the value estimation function of Bellman error has many similarities with the supervised learning. We use a stochastic gradient based Stochastic Gradient Langevin Dynamics (SGLD) to achieve posterior sampling \cite{SGLD}. This work uses the Deep Deterministic Policy Gradient (DDPG) algorithm as the baseline algorithm \cite{DDPG}. We use the SGLD sampler instead of the value estimate function's optimizer to continuously generate a posteriori samples of the current observations during the training step, and use these samples to generate policy gradient optimization actors for exploration during the rollout phase. Further, we consider the parts of the original algorithm that are not coordinated with the posterior sampling and adjust them to obtain a more uniform sample. This is a more accurate way to sample from the posterior and introduces little additional computational cost.Further, we adjusted the portion of the baseline algorithm that is not coordinated with the posterior sampling to achieve a more consistent posteriori sampling. This is a more accurate way to sample from the posterior and introduces little additional computational cost.

In this work, we validated the performance of our exploration method with continuous MountainCar, Sparse HalfCheetah, and HalfCheetah as deceptive, sparse, and well-defined tasks, and compared it with Action noise, Parameter noise. In general, SGLD-based approaches can solve a wider variety of tasks with uniform settings and get better results in well-defined tasks. We summarize the three main contributions of this work:
\begin{itemize}
\item  It is the first time to propose the use of the SGLD method to achieve deep exploration in RL which use DNN as a value estimate function.
\item  The method of using SGLD sampler to do Thomson sampling in DDPG algorithm is proposed. No hyperparameter is introduced, and there is almost no additional computational overhead.
\item This work proves through experiments that the method of this paper can solve the three different tasks of sparse reward, deception reward and good definition by using uniform settings.
\end{itemize} 

This paper is organized as follows: In the section 2, we summarize some existing exploration methods and explain the relationship between our work and them. The third section briefly introduces the DDPG algorithm and the principle of SGLD, and gives a detailed description of how we apply SGLD in DDPG. In Section 4 we conducted experiments on several representative tasks and analyzed why our methods are effective. Finally, we summarize this work in section 5 and point out our opinions on further research work.
