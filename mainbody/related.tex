\section{Related works}
Reinforcement learning algorithms use neural networks as Q-value functions or policy functions achieve remarkable results in the continuous state-action space tasks . In order to solve more difficult tasks such as sparse or deceptive reward tasks, more effective exploration strategies have become a key point.

Some naive random search algorithms are generally used as default exploration strategies by various RL algorithms. Such as the $\epsilon$-greed method which takes non-optimal action according to probability $\epsilon$, or directly superimposes noise (correlation or not) in action space which called action space noise method. Another approach is to inject noise in the parameter space of the Q-value function of the policy function. In this situation, the agent will take the same action in the face of the same state within an episode, which leads to more consistent exploration behavior. However, such methods do not explicitly suggest how to use existing data to guide exploration, resulting in low sample efficiency. 

The Thompson sampling method, as a long-history exploration strategy, has recently returned to the perspective of deep reinforcement learning researchers. This method draws a sample from the posterior distribution of the Q-value function and makes decisions based on the random sample rather than an optimal estimation. Since it is difficult to directly calculate the posterior distribution of the neural network, the key point in the work of using Thompson sampling in reinforcement learning is the approach to achieve posterior sampling. For example, BDQN and prior-BDQN uses a multi-head network trained in bootstrapping way and treats the set of output as sampled results. VIME and SteinV PG use the reparameter trick and the stein gradient approach to implement variational inference as an approximation of the posterior distribution. These methods lack intrinsic motivation in the zero reward environment (Except prior-BDQN) and introduce considerable computational overhead. 

In this work, we use the SGLD to draw samples from the posterior distribution during training the value estimation function. There are a number of variants of SGLD, such as pre-condiction, SGHMC, SGFS, etc. The pre-condiction SGLD can be seen as a variant of RMSprop and has as almost equal number of operations, and been used as the sampler in this work. Because SGLD can be seen as accumulatng Gaussian noise during the optimization process, our method has similarities with parameter noise. The difference is that we add accumulated noise to the critic by SGLD, while they only add one-time noise to the actor when rollout samples. On the other hand, in prior-BDQN, they prepare a set of "prior function" in advance then select some of them to add on Q-value function during training, while in our algorithm there only have a single-head network with accumulated noise so that our method is more computational efficient.