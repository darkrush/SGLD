\section{Morass of Exploration}
In this section, firstly, we introduce the actor-critic algorithm briefly. Then we compare well-defined tasks with sparse/deceptive tasks, and explain why these two facts cause a morass of exploration. Finally, we make a comparison with the existing research work on exploration strategies.

We first define the basic symbols of reinforcement learning. The environment in the task of reinforcement learning is modeled as a Markov Decision Process: $<\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma>$, in which $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, in time step $t$, the agent observes state $s_t$ and takes the action $a_t$, then receive reward $r_t \sim \mathcal{R}(s_t,a_t)$ and the state transfer to $s_{t+1}\sim \mathcal{P}(s_t,a_t)$. In an episode with time step $t \in {0 \cdots T}$, the total discount return $G_T = \sum_{t=0}^{T}\gamma^t r_t$. For any given policy $\mu(s,a) = \mathbb{P}[a|s]$,  start one episode from state $s$, $\mathbb{E}_{\mu}[G|s_0=s]$ the expectation of total discount return can get from state $s$, also called state-value function $V^{\mu}(s)$. Similarly, action-value function $Q^{\mu}(s,a)$ can be defined as $\mathbb{E}_{\mu}[G|s_0=s,a_0=a]$.

\subsection{Actor-Critic Algorithm}
The actor-critic algorithm combines the advantages of the policy-based method and the value-based method to become one of the most popular reinforcement learning algorithms. This algorithm uses a parametric approach to represent the policy function or actor: $\pi(s,a|\theta^{\pi})$ and the value function or critic: $Q(s, a|\theta^{Q})$, and solves the optimal parameters through optimization algorithms such as genetic algorithm, gradient descent, etc. In the actor-critic algorithm based on the gradient descent method, the policy function is optimized along the direction of policy gradient: $\nabla_{\theta^{\pi}} J = \mathbb{E}[\nabla_{\theta^{\pi}}\log \pi(s,a)\cdot Q(s,a)]$, and the value function is typically optimized by minimizing the mean square error $(R-V(s|\theta^V))^2$ with $R = \sum_{t=0}^{T-1}\gamma^t r_i +\gamma^TV(s_T)$  A general deep actor-critic algorithm framework is described in Algorithm \ref{alg:gac}.

It has been proven that using advantage function: $A(s,a)=Q(s,a)-V(s)$ instead of $Q(s,a)$ in policy gradient can reduce the variance and introduce no bias. Through the advantage function we can generate the intuition of how the critic affects the direction of policy gradient: In any state $s^*$, for any action $a^*$ with $A(s^*,a^*) > 0$, the policy gradient will increase the probability of action $a^*$ at state $s^*$ $\pi(s^*,a^*|\theta^{\pi})$. In other words, the agent will have a higher probability of selecting the most advantageous action in the current state. This intuition will help us understand the following part of this article about exploring in difficult environments.

\begin{algorithm}[htbp]
    \caption{General Deep Actor Critic Framework}
    \label{alg:gac}
 \begin{algorithmic}
    \STATE {\bfseries Input:} environment $E$, critic $V(s|\theta^V)$, actor $\pi(s|\theta^\pi)$, exploration strategy $\tilde\pi \leftarrow f(\pi)$
    \FOR{1 {\bfseries to} cycle\_number}
    \STATE Apply exploration strategy $\tilde\pi \leftarrow f(\pi)$
    \STATE Rollout data $\{d_t\}$ from $E$ by $\tilde\pi$
    \FOR{1 {\bfseries to} update\_number}
    \STATE Update Critic $V(s|\theta^V)$ by $\nabla_{\theta^V}(R-V(s|\theta^V))^2$
    \STATE Update Actor $\pi(s|\theta^\pi)$ by policy gradient 
    \ENDFOR
    \ENDFOR
 \end{algorithmic}
 \end{algorithm}



\subsection{Tasks With Different Reward Shape}
\label{sec:tasks}
Usually a reward for a task is defined according to the needs of the application. For example, in the HalfCheetah task \cite{mujoco}, a half-body cheetah with 6 degrees of freedom is placed on the ground plane. In each time step, the reward is $r_t = r_{speed} + r_{control}$, where $r_{speed}$ is proportional to the distance to the right in this time step, and $r_{control} = -\alpha*||\vec{a_t}||^2_2$ ($\alpha = 0.1$ as default) is the action penalty. The implication behind the reward is that the agent is expected to achieve higher speed with minimal control cost. Since the agent is constrained to a two-dimensional vertical plane, any random action can cause the agent to move to the right and immediately get positive feedback. This means it is easy to sample a policy which can get a positive reward from policy space, and the positive reward will further drive the agent to learn how to run faster, thus forming positive feedback.

However, this well-defined situation does not always appear in all tasks, and positive feedback may not be easy to reach. In some tasks where the goal is to reach certain target states, a positive reward can only be obtained when the agent reaches these states. This means that the agent can only get 0 rewards before it happens to touch these states. The $Q(s,a)$ or $V(s)$ will learn to equal zero for any state and action, and make the policy gradient to equal zero, the actor can not be optimized anymore. In addition, in the control task, in order to reduce the energy consumed by controlling each joint, the action penalty term (as $r_{control}$ in HalfCheetah) will be included in the reward. Before getting any positive feedback, the agent will learn to keep the zero action as the best policy to minimize the penalty, this further prevents the agent from making various exploratory actions. "Sparse reward" and "zero action" together led to the morass of exploration.In these cases, additional exploration strategies become especially important. Once the agent reaches the goal states and gains a positive reward through an additional exploration strategy, the critic will direct the actor learn to access the goal states with a higher frequency and finally learn to solve the task.

\subsection{Exploration Strategies}
The previous section briefly describes two factors that make the environment difficult to explore, sparse rewards and heavy penalty. The application requirement of "let the agent learn to achieve the goal with minimal cost" naturally leads to these two factors. In practical applications, due to insufficient understanding of the task or strict energy restrictions, it may lead to a task with sparse or deceptive nature. How to design an effective exploration strategy, so that agents can break through these difficulties, and quickly converge to the optimal policy becomes the key point to solve this kind of problem.

Some naive random search algorithms are generally used as default exploration strategies by various RL algorithms. Such as the $\epsilon$-greed method which takes non-optimal action according to probability $1-\epsilon$ \cite{DQN}, or directly superimposes noise (correlation or not) in action space \cite{DDPG}. Another approach is to inject noise in the parameter space of the Q-value function of the policy function \cite{pnoise}. In this situation, the agent will take the same action in the face of the same state within an episode, which leads to more consistent exploration behavior. However, such methods do not explicitly suggest how to use existing data to guide exploration, resulting in low sample efficiency. 

The Thompson sampling method, as an exploration strategy with a long history, has recently returned to the perspective of deep reinforcement learning researchers. This method draws a sample from the posterior distribution of the Q-value function and makes decisions based on the random sample rather than an optimal estimation. Since it is difficult to directly calculate the posterior distribution of the neural network, the key point in the work of using Thompson sampling in reinforcement learning is the approach to achieve posterior sampling. For example, BDQN \cite{BDQN} and prior-BDQN \cite{osband2018randomized} uses a multi-head network trained in bootstrapping way and treats the set of output as sampled results. VIME \cite{VIME} and Stein Variational Policy Gradient \cite{liu2017stein} use the reparameter trick and the stein gradient approach to implement variational inference as an approximation of the posterior distribution. 

A serial "sanity checks" have been proposed in prior-BDQN, including: 1.\textbf{Posterior Concentration}: the uncertainty of the posterior distribution of the value function decreases as the amount of observation data increases. 2.\textbf{Multi-step Uncertainty}: requires that the uncertainty can be propagated through bellman equation. 3.\textbf{Epistemic vs Aleatoric}: requires figure the posterior distribution of value function rather than the reward. 4.\textbf{Task-appropriate}: requires that the posterior distribution is on the parameter space of the q-function instead of the state space. 5.\textbf{Intrinsic motivation}: requires that the value-function can give a non-zero value estimate of the unknown state with only zero reward received. The last point is the key factor in whether the exploration strategy is effective in sparse reward tasks. Prior-BDQN, relative to BDQN, adds a random prior function to the value function when optimizing the bellman error, which results in the intrinsic motivation. Coincidentally, in the work of parameter noise, adding random Gaussian noise directly to the Q-function or the actor can also lead to effective exploration, even without any clear considerations for Bayesian learning. Combining these facts, we hypothesize that "noise on parameters" + "posterior sampling" may be important to the exploration strategy of deep reinforcement learning.