\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{TF}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pp.\  265--283, 2016.

\bibitem[Azizzadenesheli et~al.(2018)Azizzadenesheli, Brunskill, and
  Anandkumar]{lastLayerBayes}
Azizzadenesheli, K., Brunskill, E., and Anandkumar, A.
\newblock Efficient exploration through bayesian deep q-networks.
\newblock \emph{arXiv preprint arXiv:1802.04412}, 2018.

\bibitem[Bakker(2002)]{RN395}
Bakker, B.
\newblock Reinforcement learning with long short-term memory.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1475--1482, 2002.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{count2}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1471--1479, 2016.

\bibitem[Chen et~al.(2015)Chen, Li, Li, Lin, Wang, Wang, Xiao, Xu, Zhang, and
  Zhang]{MXNet}
Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
  C., and Zhang, Z.
\newblock Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock \emph{arXiv preprint arXiv:1512.01274}, 2015.

\bibitem[Colas et~al.(2018)Colas, Sigaud, and Oudeyer]{colas2018gep}
Colas, C., Sigaud, O., and Oudeyer, P.-Y.
\newblock Gep-pg: Decoupling exploration and exploitation in deep reinforcement
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1802.05054}, 2018.

\bibitem[Conti et~al.(2018{\natexlab{a}})Conti, Madhavan, Such, Lehman,
  Stanley, and Clune]{ERL2}
Conti, E., Madhavan, V., Such, F.~P., Lehman, J., Stanley, K., and Clune, J.
\newblock Improving exploration in evolution strategies for deep reinforcement
  learning via a population of novelty-seeking agents.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5032--5043, 2018{\natexlab{a}}.

\bibitem[Conti et~al.(2018{\natexlab{b}})Conti, Madhavan, Such, Lehman,
  Stanley, and Clune]{conti2018improving}
Conti, E., Madhavan, V., Such, F.~P., Lehman, J., Stanley, K., and Clune, J.
\newblock Improving exploration in evolution strategies for deep reinforcement
  learning via a population of novelty-seeking agents.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5032--5043, 2018{\natexlab{b}}.

\bibitem[Henderson et~al.(2017)Henderson, Doan, Islam, and
  Meger]{dropoutInference}
Henderson, P., Doan, T., Islam, R., and Meger, D.
\newblock Bayesian policy gradients via alpha divergence dropout inference.
\newblock \emph{arXiv preprint arXiv:1712.02037}, 2017.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, De~Turck, and
  Abbeel]{VIME}
Houthooft, R., Chen, X., Duan, Y., Schulman, J., De~Turck, F., and Abbeel, P.
\newblock Vime: Variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1109--1117, 2016.

\bibitem[Khadka \& Tumer(2018)Khadka and Tumer]{EPGRL}
Khadka, S. and Tumer, K.
\newblock Evolution-guided policy gradient in reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1196--1208, 2018.

\bibitem[Lehman \& Stanley(2011)Lehman and Stanley]{lehman2011abandoning}
Lehman, J. and Stanley, K.~O.
\newblock Abandoning objectives: Evolution through the search for novelty
  alone.
\newblock \emph{Evolutionary computation}, 19\penalty0 (2):\penalty0 189--223,
  2011.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, and
  Wierstra]{DDPG}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{Computer Science}, 8\penalty0 (6):\penalty0 A187, 2015.

\bibitem[Liu et~al.(2017)Liu, Ramachandran, Liu, and Peng]{liu2017stein}
Liu, Y., Ramachandran, P., Liu, Q., and Peng, J.
\newblock Stein variational policy gradient.
\newblock \emph{arXiv preprint arXiv:1704.02399}, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, and Ostrovski]{DQN}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., and Ostrovski, G. J.~N.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.
\newblock ISSN 1476-4687.

\bibitem[OpenAI(2018)]{OpenAIdota}
OpenAI.
\newblock Openai five.
\newblock \url{https://blog.openai.com/openai-five/}, 2018.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Van~Roy]{BDQN}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4026--4034, 2016.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Osband, I., Aslanides, J., and Cassirer, A.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.03335}, 2018.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{PyTorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Plappert et~al.(2017)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{pnoise}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock \emph{arXiv preprint arXiv:1706.01905}, 2017.

\bibitem[Riquelme et~al.(2018)Riquelme, Tucker, and Snoek]{Showdown}
Riquelme, C., Tucker, G., and Snoek, J.
\newblock Deep bayesian bandits showdown.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband, Wen,
  et~al.]{TStutorial}
Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., Wen, Z., et~al.
\newblock A tutorial on thompson sampling.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (1):\penalty0 1--96, 2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, and
  Lanctot]{AlphaGO}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., and Lanctot, M.
  J.~n.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484, 2016.
\newblock ISSN 1476-4687.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  DeTurck, and Abbeel]{count1}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2753--2762, 2017.

\bibitem[Thompson(1933)]{TS}
Thompson, W.~R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{rmsprop}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Welling \& Teh(2011)Welling and Teh]{SGLD}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pp.\  681--688, 2011.

\end{thebibliography}
